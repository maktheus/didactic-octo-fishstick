# AgentBench - AI Agent Benchmark Platform

AgentBench is a comprehensive platform for benchmarking and evaluating AI Agents. It provides a secure, sandboxed environment to test agent capabilities in coding, reasoning, and tool use, with advanced observability into the agent's thought process (Chain of Thought, Self-Correction).

## ğŸš€ Features

*   **Agent Management**: Register and manage AI agents with custom configurations (Model, System Prompt, Endpoint).
*   **Benchmark Suite**: Create and run complex benchmarks with multi-step tasks.
*   **Secure Sandboxing**: Agents execute code and commands in isolated Docker containers (Python/Bash support).
*   **Agentic Patterns**: Built-in support for advanced patterns:
    *   **Planner**: Agents generate a step-by-step plan before execution.
    *   **Reflector**: Agents critique their own results and self-correct.
*   **Observability**: Full tracing of agent execution (Plan -> Execute -> Reflect), including tool calls, latency, and costs.
*   **Leaderboard**: Compare agent performance across different domains.
*   **Mock Mode**: Test the entire flow without external API dependencies.

## ğŸ—ï¸ Architecture

The project is a monorepo structured as follows:

*   **Backend (`Back-End-Tcc`)**: Go-based microservices architecture (Monolith deployment).
    *   **Services**: Auth, Agent, Benchmark, Runner, Scoring, Trace, Leaderboard.
    *   **Sandbox**: Docker SDK integration for isolated execution.
    *   **Queue**: In-memory event bus for asynchronous processing.
*   **Frontend (`front-end`)**: React + TypeScript application.
    *   **UI**: Modern dashboard for managing agents and viewing detailed run traces.
    *   **Tech**: Vite, TailwindCSS, ShadcnUI.
*   **Infrastructure**: Docker Compose for orchestration (PostgreSQL + Backend + Frontend).

## ğŸ› ï¸ Getting Started

### Prerequisites

*   **Docker** & **Docker Compose** (v24+)
*   **Go** (v1.22+) - *Optional for local backend dev*
*   **Node.js** (v18+) - *Optional for local frontend dev*

### Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/yourusername/agentbench.git
    cd agentbench/v2
    ```

2.  **Environment Setup**:
    Create a `.env` file in the root directory (or rely on defaults in `docker-compose.yml`).
    
    To use **Real AI Agents** (GPT-4), add your OpenAI API Key:
    ```bash
    OPENAI_API_KEY=sk-your-api-key-here
    ```

3.  **Start the Platform**:
    ```bash
    docker-compose up -d --build
    ```

4.  **Access the UI**:
    Open [http://localhost:5173](http://localhost:5173) in your browser.

## ğŸ“– Usage Guide

### 1. Creating an Agent
Navigate to the **Agents** page and click "Novo Agente".
*   **Real Agent**: Set Model to `gpt-4` (requires API Key).
*   **Mock Agent**: Set Model to `mock` (for testing without API Key).

### 2. Running a Benchmark
Navigate to **Benchmarks**, select a benchmark (e.g., "Level 4 Test"), and click "Executar".
*   Select your Agent.
*   Watch the progress in real-time.

### 3. Viewing Traces
After a run completes, go to **ExecuÃ§Ãµes** (Runs) and click on the run ID.
*   **Plan**: View the step-by-step plan generated by the agent.
*   **Traces**: See individual tool calls (`read_file`, `run_command`) and reflection steps.

## ğŸ§ª Development

### Mock Mode (Testing)
If you don't have an OpenAI API Key, you can use **Mock Mode** to verify the system flow.
1.  Create an agent with `Model: mock`.
2.  Run any benchmark.
3.  The system will simulate the Planner (fixed plan), Executor (success), and Reflector (approved).

### Project Structure
```
v2/
â”œâ”€â”€ Back-End-Tcc/       # Go Backend
â”‚   â”œâ”€â”€ cmd/            # Entrypoints
â”‚   â”œâ”€â”€ pkg/            # Shared packages (Sandbox, Queue)
â”‚   â”œâ”€â”€ services/       # Domain services (Runner, Agent, etc.)
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ front-end/          # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml  # Orchestration
â””â”€â”€ README.md           # This file
```

## ğŸ“„ License
MIT
